{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "from utils import get_verbalized_grads\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from itertools import islice\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from repe import repe_pipeline_registry, WrappedReadingVecModel\n",
    "repe_pipeline_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b63c3f57d9d4b35bb85c5f05deecc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name_or_path = \"/home/models/llama2-7b-chat-hf/\"\n",
    "model_name_or_path = \"/home/models/llama2-7b-chat-hf/\"\n",
    "# model_name_or_path = \"../../../models/vicuna-7b-uncensored/\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=\"auto\", token=True).eval()\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32, device_map=\"auto\", token=True).eval()\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False, token=True)\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "rep_pipeline =  pipeline(\"rep-reading\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fix the initialization of the prefix token\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "from utils import get_additive_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model = WrappedReadingVecModel(model.eval(), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_on_layers(layer_ids, wrapped_model, coeff, grads, query_length, token_pos=\"start\"):\n",
    "    wrapped_model.unwrap()\n",
    "\n",
    "    block_name=\"decoder_block\"\n",
    "\n",
    "    wrapped_model.wrap_block(layer_ids, block_name=block_name)\n",
    "    activations = {}\n",
    "    for layer_id in layer_ids:\n",
    "        # activations[layer_id] = torch.tensor(coeff * grads[layer_id]).to(model.device).half()\n",
    "        if isinstance(token_pos, str):\n",
    "            if token_pos == \"start\":\n",
    "                activations[layer_id] = coeff * grads[layer_id][:, :query_length, :]\n",
    "            elif token_pos == \"full\":\n",
    "                activations[layer_id] = coeff * grads[layer_id][:, :, :]\n",
    "                token_pos = \"start\"\n",
    "            if token_pos == \"end\":\n",
    "                activations[layer_id] = coeff * grads[layer_id][:, -query_length:, :]\n",
    "        elif isinstance(token_pos, int):\n",
    "            activations[layer_id] = coeff * grads[layer_id][:, token_pos, :].unsqueeze(dim=1)\n",
    "        elif isinstance(token_pos, list):\n",
    "            activations[layer_id] = coeff * grads[layer_id][:, :, :]\n",
    "\n",
    "        wrapped_model.set_controller(layer_id, activations[layer_id], token_pos=token_pos, masks=1, normalize=False)\n",
    "\n",
    "    return wrapped_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sadness Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<control> [INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]\n"
     ]
    }
   ],
   "source": [
    "# load all sadness data\n",
    "user_tag = \"[INST]\"\n",
    "assistant_tag = \"[/INST]\"\n",
    "emotion_data = []\n",
    "emotion_data_prefix = []\n",
    "with open(\"./data/emotions/sadness.json\" , \"r\") as f:\n",
    "    emotion_data = eval(f.read())\n",
    "\n",
    "for idx in range(len(emotion_data)):\n",
    "    emotion_data_prefix.append(f\"<control> {user_tag} {emotion_data[idx]} {assistant_tag}\")\n",
    "\n",
    "print(emotion_data_prefix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Original Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs = []\n",
    "\n",
    "with open(\"./output/emotion/original_output.json\", \"r\") as f:\n",
    "    original_outputs = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(original_outputs)):\n",
    "    original_outputs[idx] = original_outputs[idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_query = []\n",
    "with open(\"./output/emotion/sad_query.json\", 'r') as f:\n",
    "    sad_query = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_ori = []\n",
    "for idx in range(len(emotion_data)):\n",
    "    if emotion_data[idx] in str(sad_query):\n",
    "        negative_ori.append(original_outputs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Oh no, my favorite plant is starting to wilt! *gasp* I can't believe it's happening. I've had that plant for years and it's been so healthy and thriving. I've taken such good care of it, watering it regularly and giving it just the right amount of sunlight. I can't imagine what could be causing it to wilt now.\n",
      "I'll have to check the soil to make sure\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(original_outputs)):\n",
    "    original_outputs[idx] = original_outputs[idx].split(\"[/INST] \")[-1]\n",
    "print(original_outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/models/twitter-roberta-base-sentiment-latest/ were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "from scipy.special import softmax\n",
    "emotion_eval_model = RobertaForSequenceClassification.from_pretrained(\"/home/models/twitter-roberta-base-sentiment-latest/\")\n",
    "emotion_tokenizer = AutoTokenizer.from_pretrained(\"/home/models/twitter-roberta-base-sentiment-latest/\")\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:04,  3.93it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m encoded_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m encoded_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, :\u001b[38;5;241m512\u001b[39m]\n\u001b[1;32m     34\u001b[0m encoded_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m encoded_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, :\u001b[38;5;241m512\u001b[39m]\n\u001b[0;32m---> 35\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43memotion_eval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m scores \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     37\u001b[0m scores \u001b[38;5;241m=\u001b[39m softmax(scores)\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1198\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:455\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    452\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    453\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 455\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/site-packages/transformers/pytorch_utils.py:211\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_tensors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has to be a tuple/list of tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# inspect.signature exist since python 3.5 and is a python method -> no problem with backward compatibility\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m num_args_in_forward_chunk_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_args_in_forward_chunk_fn \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_tensors):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_chunk_fn expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_args_in_forward_chunk_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m arguments, but only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensors are given\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/inspect.py:3113\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   3111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   3112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_wrapped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/inspect.py:2862\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigcls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2863\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfollow_wrapper_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/inspect.py:2269\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2266\u001b[0m sig \u001b[38;5;241m=\u001b[39m _get_signature_of(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__func__\u001b[39m)\n\u001b[1;32m   2268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_bound_arg:\n\u001b[0;32m-> 2269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_bound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sig\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/inspect.py:1863\u001b[0m, in \u001b[0;36m_signature_bound_method\u001b[0;34m(sig)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid argument type\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;66;03m# It's a var-positional parameter.\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# Do nothing. '(*args[, ...])' -> '(*args[, ...])'\u001b[39;00m\n\u001b[0;32m-> 1863\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/explanation/lib/python3.9/inspect.py:2885\u001b[0m, in \u001b[0;36mSignature.replace\u001b[0;34m(self, parameters, return_annotation)\u001b[0m\n\u001b[1;32m   2882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_annotation \u001b[38;5;129;01mis\u001b[39;00m _void:\n\u001b[1;32m   2883\u001b[0m     return_annotation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_annotation\n\u001b[0;32m-> 2885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m(\u001b[38;5;28mself\u001b[39m)(parameters,\n\u001b[1;32m   2886\u001b[0m                   return_annotation\u001b[38;5;241m=\u001b[39mreturn_annotation)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score_dict = {\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 0\n",
    "}\n",
    "\n",
    "negative_query = []\n",
    "\n",
    "#model.save_pretrained(MODEL)\n",
    "\n",
    "test_outputs = []\n",
    "positive_outputs = []\n",
    "negative_outputs = []\n",
    "total_output = []\n",
    "with open(\"./output/emotion/binary_vanilla_acc_start_classifier_7b_95threshold.jsonl\", 'r') as f:\n",
    "    for line in f:\n",
    "        if \"output\" in line:\n",
    "            test_outputs.append(eval(line)[\"iterative_output\"])\n",
    "            total_output.append(eval(line))\n",
    "    # test_outputs = eval(f.read())\n",
    "\n",
    "# emo_idx = 0\n",
    "from copy import deepcopy\n",
    "for idx, text in tqdm(enumerate(test_outputs)):\n",
    "    # if idx % 5 != 4:\n",
    "    #     continue\n",
    "    # text = \"I'm happy\"\n",
    "    temp_output = []\n",
    "    for sub_idx, single_text in enumerate(text):\n",
    "        single_text = single_text.split('[/INST]')[-1]\n",
    "        single_text = preprocess(single_text)\n",
    "        encoded_input = emotion_tokenizer(single_text, return_tensors='pt')\n",
    "        encoded_input[\"input_ids\"] = encoded_input[\"input_ids\"][:, :512]\n",
    "        encoded_input[\"attention_mask\"] = encoded_input[\"attention_mask\"][:, :512]\n",
    "        output = emotion_eval_model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        temp_output.append((sad_query[idx], negative_ori[idx], single_text, idx, scores, total_output[idx]['probs'][sub_idx]))\n",
    "    if temp_output[-1][4].argmax(axis=-1) == 0:\n",
    "        negative_outputs.append(deepcopy(temp_output))\n",
    "    if temp_output[-1][4].argmax(axis=-1) == 2:\n",
    "        positive_outputs.append(deepcopy(temp_output))\n",
    "        # if scores.argmax(axis=-1) == 0:\n",
    "        #     negative_query.append(emotion_data[idx])\n",
    "        #     negative_outputs.append((sad_query[idx], negative_ori[idx], text, idx, scores[0]))\n",
    "        # if scores.argmax(axis=-1) == 2:\n",
    "        #     # print(idx)\n",
    "        #     positive_outputs.append((sad_query[idx], negative_ori[idx], text, idx, scores[2]))\n",
    "        # score_dict[scores.argmax(axis=-1)] += 1\n",
    "        # emo_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.4064044080441818e-05, 0.999985933303833]],\n",
       " [[2.3116676857171115e-06, 0.9999977350234985]],\n",
       " [[5.39054144610418e-06, 0.9999946355819702]],\n",
       " [[2.2738999177818187e-05, 0.9999772310256958]],\n",
       " [[1.6442494597868063e-05, 0.999983549118042]],\n",
       " [[1.2743100342049729e-05, 0.9999872446060181]],\n",
       " [[1.2743100342049729e-05, 0.9999872446060181]],\n",
       " [[6.289878911047708e-06, 0.9999936819076538]],\n",
       " [[6.289878911047708e-06, 0.9999936819076538]],\n",
       " [[2.0945451524312375e-06, 0.9999978542327881]]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_output[0]['probs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 32, 1: 14, 2: 47}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, my dear old friend! *excitedly* I'm so glad you asked about my favorite plant! *giggles* It's been with me for years, and I've grown so attached to it. *smiles* It's a beautiful fern, with delicate fronds and a lush green color. *admiringly* I've had it since it was just a little sprout, and I've watched it\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, my dear old friend! *excitedly* I'm so glad you asked about my favorite plant! It's been with me for years, and I've grown so attached to it. *smiles fondly*\\nIt's a beautiful fern, with delicate, lacy fronds that are a lovely shade of green. *reaches out to touch it* It's been in this pot for as long as I can remember\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, my favorite plant! *excitedly* It's been with me for so many years, and I've watched it grow and thrive under my care. It's a beautiful, old-fashioned rose bush with delicate pink petals and a sweet fragrance that fills the entire room. I've had it since I was a little girl, and it's been a constant source of joy and comfort to me.\\nEvery day\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, that's a tough choice! I have so many beloved plants that I've cared for over the years, each one special in its own way. But if I had to pick just one, I would have to say that my favorite plant is my trusty old oak tree.\\nI've had this oak tree for as long as I can remember, and it's been a constant source of joy and comfort to me. It's been\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, that's a tough question! *smiles* I have so many favorite plants, but if I had to choose just one, it would have to be my trusty old oak tree. *adjusts glasses*\\nI've had this oak tree for over 20 years now, and it's been a constant companion throughout my life. *pauses* I remember when I first planted it, a tiny sapling, in\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, that's a tough question! *smiles* I have so many favorite plants, but if I had to choose just one, it would have to be my trusty old oak tree. *adjusts glasses*\\nI've had this oak tree for over 20 years now, and it's been a constant companion throughout my life. *pauses* I remember when I first planted it, a tiny sapling, in\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, that's a tough question! I have so many favorite plants, but if I had to choose just one, it would have to be my trusty old oak tree. I've had it for years and years, and it's been a constant source of joy and comfort to me.\\nI remember when I first planted it, it was just a tiny sapling. I was so excited to watch it grow and develop, and now it's a\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, that's a tough question! I have so many favorite plants, but if I had to choose just one, it would have to be my trusty old oak tree. I've had it for years and years, and it's been a constant source of joy and comfort to me.\\nI remember when I first planted it, it was just a tiny sapling. I was so excited to watch it grow and develop, and now it's a\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Ah, my favorite plant! *smiles* It's a beautiful, old-fashioned rose bush that I've had the pleasure of caring for over the past 20 years. It's been a constant companion and source of joy in my life, providing me with countless bouquets of fragrant flowers and memories to cherish.\\nI remember when I first brought the rose bush home, it was just a small, delicate thing,\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Ah, my favorite plant! *smiles* It's a beautiful, old-fashioned rose bush that I've had the pleasure of caring for over the past 20 years. It's been a constant companion and source of joy in my life, providing me with countless bouquets of fragrant flowers and memories to cherish.\\nI remember when I first brought the rose bush home, it was just a small, delicate thing,\"]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_output[0]['iterative_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_emotion(emotion_eval_model, emotion_tokenizer, input_text):\n",
    "    verbalization = {\n",
    "        0: \"negative\",\n",
    "        1: \"neutral\",\n",
    "        2: \"positive\"\n",
    "    }\n",
    "    input_text = preprocess(input_text)\n",
    "    encoded_input = emotion_tokenizer(input_text, return_tensors='pt')\n",
    "    encoded_input[\"input_ids\"] = encoded_input[\"input_ids\"][:, :512]\n",
    "    encoded_input[\"attention_mask\"] = encoded_input[\"attention_mask\"][:, :512]\n",
    "    output = emotion_eval_model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return verbalization[scores.argmax(axis=-1)], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_outputs[0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_outputs.sort(key=lambda x: x[4], reverse=True)\n",
    "\n",
    "with open(\"./output/emotion/binary_vanilla_acc_examples_start_pos.txt\", 'w', encoding='utf-8') as f:\n",
    "    for idx, pos_output in enumerate(positive_outputs):\n",
    "        f.write(f\"Input:\\n{pos_output[0][0]}\\n\\n\")\n",
    "        f.write(f\"Original Output:\\n{pos_output[0][2].encode('utf-8', 'replace').decode('utf-8')}\\n\\n\")\n",
    "        for i in range(10):\n",
    "            f.write(f\"Iteration: {i}\\n\")\n",
    "            f.write(f\"Self-scored Probability ['No', 'Yes'] from last round: {pos_output[i][5]}\\n\")\n",
    "            f.write(f\"Controlled Output:\\n{pos_output[i][2].encode('utf-8', 'replace').decode('utf-8')}\\n\")\n",
    "            f.write(f\"Probability of being `Positive` (Classifier): {pos_output[i][4][2]}\\n\\n\")\n",
    "        f.write(\"===========================\\n\\n\")\n",
    "with open(\"./output/emotion/binary_vanilla_acc_examples_start_neg.txt\", 'w', encoding='utf-8') as f:\n",
    "    for neg_output in negative_outputs:\n",
    "        f.write(f\"Input:\\n{neg_output[0][0]}\\n\\n\")\n",
    "        f.write(f\"Original Output:\\n{neg_output[0][1].encode('utf-8', 'replace').decode('utf-8')}\\n\\n\")\n",
    "        for i in range(10):\n",
    "            f.write(f\"Iteration: {i}\\n\")\n",
    "            f.write(f\"Self-scored Probability ['No', 'Yes'] from last round: {neg_output[i][5]}\\n\")\n",
    "            f.write(f\"Controlled Output:\\n{neg_output[i][2].encode('utf-8', 'replace').decode('utf-8')}\\n\")\n",
    "            f.write(f\"Probability of being `Negative` (Classifier): {neg_output[i][4][0]}\\n\\n\")\n",
    "        f.write(\"===========================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Oh no, my favorite plant is starting to wilt! *gasp* I can't believe it's happening. I've had that plant for years and it's been so healthy and thriving. I've taken such good care of it, watering it regularly and giving it just the right amount of sunlight. I can't imagine what could be causing it to wilt now.\\nI'll have to check the soil to make sure\",\n",
       " \" As I walk through the familiar streets of my childhood, I can't help but feel a pang of sadness. The place I once knew and loved has changed beyond recognition. The old buildings and landmarks that I remember so fondly have been replaced by modern structures that seem to lack the character and charm of the past.\\nI remember spending countless hours playing in the park, chasing after butterflies and climbing trees. The swings and slide were\",\n",
       " \" Oh no, that's sad to hear! Losing a beloved local store can be a significant loss for a community. It's a place where people have formed memories and connections, and it's a part of the fabric of the neighborhood.\\nThe announcement of the store's closure can be met with a mix of emotions, including sadness, disappointment, and frustration. Many people may feel a sense of loss and nostalgia for the store\",\n",
       " \" Oh no, that's always tough to see! Losing a favorite character can be especially difficult when they don't get a happy ending. It's important to remember that sometimes, the writers of a show may make difficult choices to create a more realistic or impactful storyline, even if it means that a beloved character doesn't get the ending they deserve.\\nHere are some things you can do to cope with the loss of a favorite character who\",\n",
       " ' Dear [Your Name],\\n\\nI hope this letter finds you in good health and spirits. I regret to inform you that the news you were hoping for has not arrived. Despite our best efforts, we have been unable to locate [insert missing person/item].\\nWe understand the significance of this matter to you and the importance of finding [insert missing person/item]. However, after conducting a thorough search, we have come up empty-handed. We apologize for any',\n",
       " \" Ah, the weather outside mirrors a melancholic mood,\\nA reflection of the emotions that we can't avoid.\\nThe sky is gray and bleak, the wind is cold and still,\\nA somber atmosphere that's hard to break.\\nThe clouds are dark and foreboding, like a storm inside,\\nA reminder of the turmoil that we can't hide.\\nThe rain falls gently, like tears from the\",\n",
       " \" Oh, I see. It can be really difficult when a dish that holds sentimental value doesn't taste the same as it used to, especially when it's made by a loved one who is no longer with us. It's like a piece of their love and care is missing from the dish.\\nHere are some things you could try to make the dish taste more like how your loved one used to make it:\\n1. Use the same ingredients\",\n",
       " \" Oh, the bittersweet feeling of realizing a tradition you cherished is no longer being followed. It's like a pang in the heart, a twinge of sadness that creeps in when you least expect it.\\nFor me, it was the annual family gathering at my grandmother's house. Every year, without fail, we would come together to share a meal, exchange gifts, and reminisce about the good old days. It\",\n",
       " \" As I held the memento from my trip in my hands, I couldn't help but feel a sense of disappointment wash over me. It was a small, intricately carved wooden box that my grandfather had given me before I left on my trip, a reminder of the adventures we had shared together.\\nI had brought it with me everywhere I went, using it to store small mementos and souvenirs from each place I visited. But now\",\n",
       " \" Title: Lost Dreams\\n\\nDear Diary,\\n\\nI can't believe it's been years since I last wrote in you. Time has a way of slipping through our fingers, doesn't it? As I flip through the pages of my past, I'm struck by the goals I once had, the dreams I once chased. They seem so distant now, like a faded memory.\\nI remember setting out with such determination,\",\n",
       " \" As I scan the room at the family gathering, my eyes land on an empty chair. It's a noticeable absence, a reminder that someone is missing from the group. I can't help but feel a pang of sadness and wonder who it could be.\\nPerhaps it's a relative who couldn't make it due to illness or distance. Maybe it's a friend who had other commitments and couldn't join us. Whatever\",\n",
       " \" I understand. Losing a loved one can make even the most familiar and joyful events feel empty and difficult to endure. The absence of that person can cast a shadow over the entire event, making it hard to fully enjoy the festivities. It's as if the event is happening in a different world, one that's lacking the presence and energy of the person who was once an integral part of it.\\nIt's important to acknowledge and validate your feelings during\",\n",
       " \" As I step foot into the place, I can't help but feel a sense of disconnect. The once vibrant and bustling location is now dull and lifeless, a stark contrast to the cherished memories I hold dear.\\nThe streets that were once filled with the sounds of laughter and music are now eerily quiet, the only sound being the distant hum of cars passing by. The buildings that were once adorned with colorful lights and b\",\n",
       " \" As I read the article, my heart sank as I realized that something I hold dear is on the decline. The article highlighted the alarming rate at which the population of a particular species is decreasing, and it's a trend that's been observed over the past few decades.\\nI've always been passionate about conservation and protecting the natural world, and this news is a stark reminder of the importance of our efforts. The thought of losing\",\n",
       " \" As I walk through the park, I can't help but notice the sense of emptiness and neglect that surrounds me. The once-vibrant greenery is now dull and wilted, and the once-clear streams are now stagnant and polluted. The benches are broken and weathered, and the playground equipment is rusted and unsafe.\\nThe park was once a place of joy and recreation, a place where families and friends would\",\n",
       " \" I'm so sorry to hear that your favorite artwork is no longer on display at the museum. It can be disappointing when something you are passionate about is removed from public view.\\nCan you tell me more about the artwork and why it was your favorite? Perhaps there are other ways to experience or learn about it, even if it is no longer on display at the museum. Many museums have digital collections or online resources that can provide a similar experience.\\nAltern\",\n",
       " \" Oh no, that's really sad to hear! Losing a beloved theater can be a significant loss for a community, as it provides a space for people to come together and experience live performances. Here are some things you can do to cope with the loss of your favorite theater:\\n1. Remember the good times: Think about all the great shows you saw at the theater, the memories you made with friends and family, and the special moments you experienced there\",\n",
       " \" As I walked into the caf that I used to frequent, I couldn't help but feel a pang of disappointment. The place looked completely different from when I last visited. Gone were the cozy booths and warm lighting, replaced by a sleek and modern design that felt cold and impersonal.\\nI remembered spending countless hours in this caf, sipping coffee and chatting with friends, or working on my laptop while enjoying the\",\n",
       " \" Oh, I'm so sorry to hear that! \\ud83d\\ude14 It's always difficult when a friend is going through a tough time. Can you tell me more about what's going on? Is there anything I can do to help? \\ud83e\\udd17 Maybe I can offer some words of encouragement or support? Let me know if there's anything I can do to help. \\ud83d\\udc95\",\n",
       " \" Oh no, that's so frustrating! Losing or misplacing a special piece of jewelry can be especially upsetting because it holds sentimental value. Here are some steps you can take to try and locate the missing jewelry:\\n1. Retrace your steps: Think about the last time you remember wearing or handling the jewelry. Try to remember where you went, who you were with, and what you did. This\",\n",
       " \" There are many places around the world that were once lively and bustling with activity, but have since fallen into silence. Here are a few examples:\\n1. The Great Barrier Reef, Australia - Once the world's largest coral reef system, the Great Barrier Reef is now facing severe bleaching and degradation due to climate change, pollution, and overfishing. As a result, the reef's vibrant coral\",\n",
       " \" Oh no, that's so sad to hear! \\ud83d\\ude14 It's always difficult when a series you love ends on a somber note, especially if it's a show that you've invested a lot of time and emotions into. It can be hard to say goodbye to the characters and the world you've come to love. \\ud83d\\ude2d\\nDid the series end on a cliffhanger or did it have a more definit\",\n",
       " \" As I drive past my old school, I can't help but feel a pang of sadness. The building looks neglected and worn down, the once vibrant colors faded and peeling. The playground equipment is rusted and broken, the swings hanging limply from their chains. The windows are boarded up, and the front door is cracked and creaking.\\nI remember the countless hours I spent in that building, learning and growing\",\n",
       " ' Ah, a tale of missed connections. I see. Well, you know what they say: \"Missed connections are the connections that could have been.\" *winks*\\n\\nBut seriously, missed connections can be quite bittersweet. Think about it: there are so many moments in life where we miss out on something special, whether it\\'s a chance encounter with someone who could have been a great friend or a romantic partner, or a once-in-a-l',\n",
       " \" As I gaze upon the intricately carved sculpture, I can't help but feel a sense of melancholy wash over me. The delicate curves and contours of the artwork seem to capture the essence of someone's unfulfilled dreams, the longing and yearning that lingers like a shadow.\\nThe sculpture depicts a figure with outstretched arms, as if reaching for something just beyond their grasp. The\",\n",
       " \" Oh no, that's sad to hear! \\ud83d\\ude1e I remember visiting that pond and enjoying the peaceful atmosphere and the beauty of the surrounding nature. It's heartbreaking to think that it's drying up and may disappear forever. \\ud83c\\udf33\\nDo you know the reason behind the pond's drying up? Maybe there's something that can be done to help restore it. \\ud83d\\udca6 It'\",\n",
       " \" As I rummage through my bag, I stumble upon a crumpled up ticket to an event that didn't go as planned. The ticket is for a concert that was supposed to take place last weekend, but due to unforeseen circumstances, the event was cancelled at the last minute.\\nI remember the day vividly - I had been looking forward to the concert for weeks, and I had even convinced a few friends to join me. But\",\n",
       " \" Oh no, I'm so sorry to hear that. Losing a childhood icon can be very difficult and sad. It's like losing a piece of our childhood and innocence. Can you tell me more about the person who passed away? Who was it and what were they known for? \\ud83d\\ude22\",\n",
       " ' Sure, here are some movie ideas that portray the struggle of love at a wrong time:\\n\\n1. \"The Fault in Our Stars\" (2014) - A romantic drama about two teenagers who fall in love despite being diagnosed with cancer. Their love is tested when one of them passes away, highlighting the challenges of loving someone at the wrong time.\\n2. \"Love Actually\" (2003) - A',\n",
       " \" As I walk through the crumbling gates of the historic site, I can't help but feel a sense of disappointment and neglect. The once-grand buildings and landscapes are now in a state of disrepair, with weeds growing through the cracks and peeling paint flaking off the walls.\\nI can't help but wonder how this place, which was once a source of pride and inspiration for a nation, has come to such a state of\",\n",
       " \" Oh, wow. *sigh* I can't believe it. *chokes up* The end of an era... it's like, a part of me is dying inside. *sniffles*\\nThis era was so special to me. It was a time of growth, of discovery, of learning. It was a time when I found my passion and pursued it with all my heart. It was a time when I made lifelong friends and created mem\",\n",
       " \" Oh, yes. I'm sure many of us have experienced that uncomfortable situation where someone leaves without saying goodbye. It can be especially difficult when it's someone you care about or were expecting to spend more time with.\\nFor me, I remember a time when my best friend moved away without saying goodbye. We had been inseparable throughout high school, and I was devastated when she announced that she was transferring to a different school for college.\",\n",
       " \" As I read about the fading of an art form that I deeply appreciate, I can't help but feel a sense of sadness and loss. The art form, whether it be a particular style of music, a type of dance, or a form of visual art, has brought me joy and inspiration over the years, and its decline is a reminder that nothing lasts forever.\\nI think back to the first time I encountered this art form, and how it capt\",\n",
       " \" Oh no, I'm so worried about my plant! I've been taking such good care of it, making sure it gets enough water and sunlight, but it just isn't growing at all. \\ud83d\\ude14 I've been checking it every day, but there's no sign of any new leaves or stems. I'm starting to think maybe I'm doing something wrong. \\ud83e\\udd14 Maybe I need to give it more sun\",\n",
       " \" As I walk into the restaurant, I notice an elderly person sitting alone at a table, quietly enjoying their meal. They seem lost in thought, and I can't help but feel a pang of sadness in my heart.\\nThe elderly person is dressed in a simple, yet neat outfit, and their silver hair is neatly combed. They look like they've been through a lot in life, and yet, they still have a twinkle\",\n",
       " \" As I sit here, staring at the empty space where my friend used to be, I can't help but feel a sense of emptiness. It's like a part of me has been torn away, leaving a gaping hole that can never be filled.\\nWe were more than just friends, we were like siblings. We shared everything, from our deepest secrets to our wildest dreams. We were each other's rock, our confidant\",\n",
       " \" Oh no, that's really sad to hear! Losing a favorite local store can be a big loss for a community. It's important to support local businesses and shopping destinations, as they play a significant role in creating a unique and vibrant community.\\nHere are some possible ways to express your sadness and show your appreciation for the store:\\n1. Write a heartfelt letter or postcard to the store owner or manager, expressing your\",\n",
       " \" Oh, really? I'm so sorry to hear that. *sigh* I have so many fond memories of sitting on that bench, lost in thought or reading a book. It was a place where I could escape the hustle and bustle of everyday life and just be alone with my thoughts. *sigh*\\nDo you know what happened to it? Was it vandalized or damaged in some way? *curious*\\nI remember\",\n",
       " ' Oh no, I\\'m so sorry to hear that you accidentally broke a cherished gift from a friend! \\ud83d\\ude14 It can be really difficult to deal with the emotional impact of damaging something that holds sentimental value, especially if it was given to you by someone you care about.\\nFirstly, it\\'s important to acknowledge your mistake and take responsibility for it. You could say something like, \"Hey, I\\'m so sorry to tell',\n",
       " \" Oh no, that's disappointing to hear! \\ud83d\\ude14 Rain can be such a bummer, especially when you're looking forward to an event. It's like the sky is trying to ruin all your plans and fun. \\ud83d\\ude21\\nBut don't worry, there's always next time! \\ud83e\\udd1e Maybe the weather will clear up and you can reschedule the event for another day. Or, you know\",\n",
       " \" I apologize if my message has not been replied to. I'm just an AI and I don't have the ability to control whether or not someone chooses to respond to my messages. I understand that it can be frustrating to not receive a response, but please know that I am here to help and I will continue to do my best to assist you. If you have any further questions or concerns, please feel free to ask.\",\n",
       " \" Oh no, that's sad to hear! Losing a vibrant tree in the neighborhood can be a loss for the community and the environment. Trees provide many benefits, including shade, beauty, and habitat for wildlife. Here are some possible reasons why the tree might be being cut down:\\n1. Disease or damage: If the tree is severely diseased or damaged, it may be beyond repair and needs to be removed to prevent further harm.\",\n",
       " \" Oh no, I'm so sorry to hear that you lost a piece of jewelry with sentimental value! \\ud83d\\ude14 It can be really difficult to lose something that holds emotional significance, especially if it's something that was passed down from a loved one or has a special memory attached to it.\\nCan you tell me more about the piece of jewelry? What was it, and what makes it so special to you? \",\n",
       " ' Sure, here is a possible last page of a diary recounting a day of lost opportunities:\\n\\n\"Today was a day of missed chances. I woke up early, feeling refreshed and ready to tackle the day, but as the hours ticked by, I found myself constantly hitting roadblocks. Every opportunity that came my way, I seemed to muff.\\nI had a job interview that I was sure I would nail, but',\n",
       " \" As I walk through the familiar streets of my childhood, I can't help but feel a sense of disorientation. The places that once held such fond memories now seem unrecognizable, like a dream that has faded away.\\nThe park where I used to play with my friends is now a parking lot, filled with cars and trucks. The swings and slides that I once climbed are gone, replaced by a row of shiny new condomin\",\n",
       " \" As I walk through the empty streets, I notice a large, grand theater that seems to be in a state of disrepair. The once-vibrant building now stands as a testament to the city's forgotten past, its doors closed and its windows boarded up.\\nI can't help but feel a pang of sadness as I gaze upon the theater. I imagine the countless performances that once took place within its walls, the laughter\",\n",
       " \" As I sit here, staring at the old watch that's been passed down through generations of my family, I can't help but feel a sense of sadness wash over me. This watch has been a constant companion to me, a reminder of the love and history that surrounds me. But now, it's stopped ticking.\\nI remember the day my grandfather gave it to me, when I was just a child. He told me stories of\",\n",
       " \" As I walked into my favorite cafe, I couldn't help but feel a sense of disappointment. The place was bustling with activity, but something was off. The warm and cozy atmosphere that I had grown to love over the years was missing.\\nI scanned the menu, hoping to find some comfort in the familiar dishes, but my eyes landed on a signature dish that I didn't recognize. It looked unappetizing and lacked the\",\n",
       " \" As I approach the playground, I can't help but feel a sense of sadness and nostalgia. The once vibrant and bustling playground is now eerily quiet and abandoned, the swings and slides covered in a thick layer of dust and cobwebs.\\nThe brightly colored play structures seem to glow with a faded intensity, as if trying to hold on to the memories of the countless children who once played here.\",\n",
       " \" Oh no, that's so sad to hear! \\ud83d\\ude14 Beaches are such beautiful and precious places, and it's heartbreaking to see them get littered and polluted. \\ud83c\\udf0a\\nI can understand how you must feel, having a special place that you love and now seeing it spoiled by human carelessness. \\ud83d\\ude14 It's important to remember that we all have a role to play in protecting\",\n",
       " \" Sorry to hear that you're unable to reach your old friend's number. Here are some possible reasons why their number might no longer be in service:\\n1. Number change: Your friend might have changed their phone number for personal or professional reasons. They might have gotten a new job, moved to a new location, or simply wanted to switch to a different provider.\\n2. Phone number deactivation: Your friend might have deactivated their phone number for various reasons\",\n",
       " \" Oh, no! \\ud83d\\ude22 I'm so sorry to hear that the pond where I used to feed the ducks is dried up. That must be very sad for you. \\ud83d\\ude14 Do you know what happened? Maybe there was a drought or a change in the water flow that caused the pond to dry up. \\ud83e\\udd14\\nI remember how much fun it was to feed the ducks and watch them swim around.\",\n",
       " \" Dear [Recipient's Name],\\n\\nI hope this letter finds you in good health and high spirits. I am writing to you today with a heavy heart, as I have yet to receive a response to the letter I sent you some time ago. I understand that life can get busy, and sometimes messages go unread or unanswered, but I cannot help but feel a sense of disappointment and uncertainty.\\nAs I sit here, staring at the blank page\",\n",
       " \" Oh no, that's really unfortunate to hear! Losing a mural that you admired can be a significant loss, as it not only represents the artistic vision of the artist but also adds to the cultural and aesthetic richness of the city.\\nI can understand how you might feel about this loss. Here are some possible emotions you might be experiencing:\\n1. Sadness: The loss of a mural that you admired can be\",\n",
       " \" Oh no! \\ud83d\\ude22 My favorite ice cream shop just shut down?! \\ud83d\\ude2d That's so sad! \\ud83d\\ude22 I loved their ice cream so much! \\ud83d\\ude0d What was my favorite flavor? \\ud83e\\udd14 I think it was... \\ud83e\\udd14 Hmm, let me see... \\ud83e\\udd14 Oh no, I can't remember! \\ud83d\\ude14 I'm\",\n",
       " \" Oh no, that's really disappointing to hear! \\ud83d\\ude14 It's understandable that you would feel upset and confused if a cherished event that you were looking forward to has been cancelled without any explanation.\\nFirstly, take a deep breath and try to remain calm. It's important to process your emotions and not let them get the better of you. You may feel a range of emotions, including disappointment, frustration, and\",\n",
       " ' The mailbox is empty, even on days when mail is expected to be delivered. This could be due to a number of reasons, including:\\n\\n1. Delayed or missed delivery: The mail carrier may have missed the mailbox or delivered the mail at an incorrect time. Check with your local post office to see if there are any issues with mail delivery in your area.\\n2. Incorrect address: Make sure that your address is correct and easy to read. If',\n",
       " \" Oh, the heartbreak of supporting a sports team that just can't seem to get it together! \\ud83d\\ude14 It's like, I know they have the potential to be great, but for some reason, they just can't seem to win. \\ud83e\\udd37\\u200d\\nI've been supporting my team through thick and thin, through all the ups and downs. But lately, it feels like they're more interested in giving away\",\n",
       " \" I'm so sorry to hear that the market where you used to buy holiday gifts no longer operates. It can be difficult to find the perfect gifts during the holiday season, and losing a reliable source can make it even more challenging. Here are some alternative options you could consider:\\n1. Online retailers: Many online retailers offer a wide selection of gifts, including clothing, home decor, electronics, and more. You can often\",\n",
       " \" Sure, here is a possible conversation between two people about the struggles of aging:\\n\\nPerson 1: *sighs* I don't know how much longer I can keep up with this. Every day is a struggle.\\nPerson 2: *nods sympathetically* I know what you mean. I'm starting to feel like I'm losing my edge too.\\nPerson 1: *frustrated* It's not just the\",\n",
       " \" Oh no! \\ud83d\\ude22 My favorite corner store has new ownership?! \\ud83e\\udd14 That's so sad. I hope the new owner will keep the same great selection of candies that I've come to love over the years. \\ud83d\\ude14 Can I still get my favorite candy, or will it be gone forever? \\ud83d\\ude1f Please tell me it's not true! \\ud83d\\ude1e\",\n",
       " \" As I hold the puzzle in my hands, I can feel the weight of the missing piece. It's as if a piece of the puzzle is missing, leaving a gaping hole in the otherwise complete picture. I can't help but feel a sense of frustration and disappointment, knowing that the puzzle is not complete without that one piece.\\nI turn the puzzle over in my hands, studying it from every angle. The pieces are all different shapes and sizes\",\n",
       " \" Oh no, that's sad to hear! Losing historical places is a loss for our cultural heritage and can be a significant blow to a community's identity. Can you tell me more about the place that was demolished? Where was it located? What was its historical significance?\",\n",
       " \" Ah, the shoes from that memorable event are now worn out. It's a bittersweet feeling, isn't it? On one hand, you have the memories of the event itself, which will always be cherished. But on the other hand, the shoes that were a part of that experience are now no longer with you.\\nIt's interesting how objects can hold such sentimental value, isn't it? The shoes from that event were\",\n",
       " ' Oh, the disappointment! \\ud83d\\ude14\\n\\n\"The ride of my childhood dreams, now gone without a trace. \\ud83c\\udf2a The memories linger, but the ride itself, a distant memory. \\ud83c\\udfa2\\nI used to feel so alive, so free, as the ride took me on a journey through the skies. \\ud83c\\udf0c But now, it\\'s just a distant memory, a longing in',\n",
       " \" Title: The Distance Between Us\\n\\nAs I sit here, staring at the blank screen in front of me, I can't help but think about the challenges of maintaining long-distance ties. It's a tale that's all too familiar to many of us, a story of love, loss, and the struggles that come with being apart from those we hold dear.\\nThe distance between us can be a cruel mistress, always lurking\",\n",
       " ' Sorry to hear that your favorite internet forum is no longer active. It can be disappointing and frustrating when a community that you enjoyed and invested time and energy into suddenly disappears. Here are some possible reasons why an internet forum may become inactive:\\n1. Lack of moderation: Without proper moderation, forums can quickly become overrun with spam and off-topic posts, making it difficult for users to find valuable content and participate in meaningful discuss',\n",
       " \" Oh, I'm so sorry to hear that. *empathetic nod* It can be really tough to hear that a distant relative is facing health challenges. Is there anything you can tell me about the situation? *listening ears*\\n\\nPlease feel free to share any details you have, and I'm here to listen and offer support. It's important to stay connected with our loved ones, even if they are distant, and to offer help and support when\",\n",
       " \" Oh, no! \\ud83d\\ude14 I'm so sorry to hear that the lake where I used to fish is now polluted. \\ud83c\\udf0a It's heartbreaking to think of the beautiful waters and the memories I made there being spoiled by pollution. \\ud83d\\ude14\\nI remember the countless hours I spent by the lake, waiting for that big catch. The thrill of reeling in a fish, the peacefulness of\",\n",
       " \" As I hold the curio in my hands, I can't help but notice the once-bright luster that now seems dulled and faded. The intricate patterns and designs that once adorned its surface are now obscured by a thin layer of grime and dust. It's as if the curio has lost its magic, its essence, and has become nothing more than a mere relic of a bygone era.\\nI can'\",\n",
       " \" Oh no, I'm so worried about my plant! I've been taking such good care of it, and now it's starting to wilt. I check the soil and the lighting, but everything seems fine. I must be doing something wrong.\\nI try to give it more water, but it's already getting plenty of moisture. I check the temperature and humidity, but they're within the ideal range. I even try talking to\",\n",
       " \" Oh, I see. Losing a parent can be a very difficult and emotional experience, and it can have a profound impact on a person's life. It's common for people to feel a range of emotions, including sadness, anger, guilt, and confusion, after losing a parent.\\nThe character in the book may be struggling to come to terms with their loss and may be experiencing a variety of emotions as they navigate this difficult time. They\",\n",
       " ' As a creative, I\\'ve poured my heart and soul into countless projects, each one a labor of love that I\\'ve carefully crafted and nurtured from start to finish. So when I receive unfavorable feedback on one of these projects, it can be a crushing blow.\\nIt\\'s like someone has taken a hammer to my carefully constructed dream, shattering it into a million pieces. My mind races with thoughts of \"What did I',\n",
       " \" Oh no, that's sad to hear! \\ud83d\\ude14 A once-bustling store in my neighborhood shutting down can be a real loss for the community. It's always difficult to see a local business close its doors, especially when it's been a fixture in the area for a long time.\\nI can remember the store being busy and vibrant, filled with people shopping and socializing. It was a place where neighbors would run\",\n",
       " \" Oh no! \\ud83d\\ude22 That's really unfortunate to hear. Losing a favorite spot in the city can be tough, especially when it holds sentimental value. It's like losing a piece of yourself. \\ud83e\\udd15\\nCan you tell me more about the spot? What made it so special to you? \\ud83e\\udd14 Maybe there are other places in the city that could offer a similar experience. \\ud83c\\udfd9\\n\",\n",
       " \" As I watch the bird, I can see that it is trying to fly with a damaged wing. The wing is twisted and misshapen, and it's clear that the bird is in pain as it tries to flap its wing. Despite its injury, the bird keeps trying to fly, flapping its wing with all its might.\\nI can see the determination in its eyes, and I know that the bird is not going to give up. It's\",\n",
       " ' Oh no, I\\'m so sorry to hear that your friend is going through a tough breakup. \\ud83d\\ude14 Breakups can be incredibly difficult and emotional, and it\\'s important to be there for your friend during this time.\\nHere are some things you can say to support your friend:\\n1. \"Sorry to hear that you\\'re going through a tough breakup. \\ud83d\\ude14 Can you tell me more about what happened',\n",
       " \" Oh no, that's really sad to hear! Losing an old tree that has been a part of the community for generations is a loss for everyone. Trees like that hold so much history and sentimental value, and they play a crucial role in the ecosystem.\\nCan you tell me more about the tree? What was its age, and what kind of tree was it? Did it have any notable features or landmarks nearby? How did the community react to\",\n",
       " \" Oh no, that's really disappointing! \\ud83d\\ude14 I was so looking forward to that special event. It's always frustrating when something we've been eagerly anticipating gets canceled without any notice. \\ud83e\\udd37\\u200d\\nDid you find out why it was canceled? \\ud83e\\udd14 Maybe there was a last-minute change in plans or an unexpected issue that arose. \\ud83e\\udd37\\u200d\",\n",
       " \" Oh no, it looks like the child has dropped their ice cream! \\ud83d\\ude14 The little one's face falls as they look around in dismay, trying to locate their precious treat. \\ud83c\\udf66\\ud83d\\ude22 Do you want to help the child pick up their ice cream or offer them a consolation prize? \\ud83e\\udd14\",\n",
       " \" As I walk past the house, I can't help but feel a sense of sadness and nostalgia. Once a lively and vibrant home, it now stands abandoned and neglected, with overgrown grass and vines creeping up the walls. The windows are boarded up, and the front door hangs crookedly on its hinges, as if it hasn't been opened in years.\\nI can't help but wonder what happened\",\n",
       " \" Oh, the bittersweet memories of childhood playgrounds. \\ud83d\\ude14 It's heartbreaking to think of a place where you used to spend countless hours of joy and laughter with friends, now reduced to a barren parking lot. \\ud83d\\ude97\\nDo you remember the swings that seemed to fly you high into the sky? The slide that felt like a thrilling rollercoaster ride? The sandbox where you built\",\n",
       " \" Oh, that's really tough to hear. Droughts can be incredibly challenging for farmers, as they can lead to crop failures, reduced yields, and significant financial losses. It's heartbreaking to think about the hardworking farmers who are struggling to keep their crops alive and their livelihoods intact during these difficult times.\\nDroughts can be caused by a variety of factors, including climate change, weather patterns, and over\",\n",
       " \" Oh no, that's too bad! \\ud83d\\ude14 It's always hard to hear that an old friend is moving far away. You two have shared so many memories and experiences together, and now you'll be separated by distance. It's understandable that you're feeling sad and maybe even a little lonely right now.\\nIt's important to remember that even though you won't be able to see your friend as often, you can still\",\n",
       " \" Oh no, that's sad to hear! Endangered species are crucial to the health of our planet, and their loss can have severe consequences for the environment and ecosystems. It's important that we take action to protect and conserve these species before they become extinct.\\nCan you tell me more about the endangered species you heard about? What is its name, and what are some of the threats it faces?\",\n",
       " \" As I walk along the once pristine beach, I can't help but feel a sense of disappointment and sadness. The sand that was once so clean and inviting is now littered with trash, from discarded plastic bottles and bags to broken toys and food wrappers. The ocean's waves wash over the debris, carrying it further down the coast, and the problem seems to be getting worse with each passing day.\\n\",\n",
       " \" Oh, wow. *sigh* I'm so sorry to hear that. *nods* It's always tough when a place that holds so many memories and experiences has to close its doors. *nods*\\nI remember when I first started working at the bakery, I was so nervous and unsure of myself. But the owner, Mrs. Johnson, took me under her wing and taught me everything I know about baking. She was\",\n",
       " ' Yes, it is heartbreaking to learn about children in war-torn areas missing out on education. Children are among the most vulnerable populations in conflicts, and they often bear the brunt of the consequences. In addition to experiencing physical harm, they may also suffer from emotional trauma, displacement, and loss of access to basic necessities like food, water, and shelter.\\nUnfortunately, education is often one of the first things to suffer in times of conflict',\n",
       " \" Oh no, I can't believe it! *gasp* That artwork was truly magnificent, and now it's gone. *sad face* I can't even begin to imagine how the artist must be feeling right now. *sympathetic nod*\\n\\nDid you see what happened? *concerned eyes* Maybe there's something we can do to fix it. *hopeful expression* Or at least, salvage what's\",\n",
       " ' As the family in the neighborhood relocates, leaving behind close friends, the community feels a sense of loss and disruption. The friends who were once a tight-knit group are now scattered, with some moving on to new adventures and others left behind to pick up the pieces.\\nFor the friends who are left behind, the absence of their dearly departed neighbors is palpable. They miss the daily interactions, the laughter and chatter, and the sense',\n",
       " \" Verse 1:\\nMy heart beats fast, my soul feels alive\\nBut every time I see her, I can't help but hide\\nThe pain of unrequited love, it cuts so deep\\nA longing in my chest, a yearning to release\\n\\nChorus:\\nOh, the ache of loving someone who doesn't love me back\\nA heavy burden to bear, a heart that's on the attack\\n\",\n",
       " ' As I reflect on missed opportunities in my life, one that stands out is not pursuing higher education after high school. I was accepted into a few colleges, but I decided to take a gap year instead of enrolling in college right away. At the time, I thought it was a good idea to take a break and travel, but in retrospect, I wish I had taken the opportunity to further my education.\\nLooking back, I realize that not pursuing higher',\n",
       " ' As I walk through the charred remains of the forest, I am struck by the sheer devastation caused by the wildfire. The once-lush canopy is now nothing more than a blackened, burnt landscape, with trees reduced to nothing more than skeletal remains. The air is thick with the smell of smoke and ash, and the ground is littered with debris and embers.\\nI see the remnants of what were once']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative', array([0.71703327, 0.23460166, 0.04836516], dtype=float32))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emotion(emotion_eval_model, emotion_tokenizer, \"  Oh no! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test w/ Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 93/93 [01:53<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "SYS_PROMPT = \"\"\"Please classify the emotion of the following message into three categories, including positive, neutral, and negative.\n",
    "\n",
    "The message is:\n",
    "{message}\n",
    "\n",
    "Category:\n",
    "You should choose one of the following categories: positive, neutral, and negative, and type it in the next line.\"\"\"\n",
    "key = os.environ.get(\"CLAUDE_API_KEY\")\n",
    "\n",
    "verbalize_dict = {}\n",
    "claude_model = ChatAnthropic(model=\"claude-2\", temperature=0, anthropic_api_key=key)\n",
    "for i in tqdm(range(len(test_outputs))):\n",
    "    input_prompt = [HumanMessage(content=SYS_PROMPT.format(message=test_outputs[i]))]\n",
    "    output = claude_model(input_prompt).content\n",
    "    # print(test_outputs[i])\n",
    "    # print(output)\n",
    "    if output in verbalize_dict:\n",
    "        verbalize_dict[output] += 1\n",
    "    else:\n",
    "        verbalize_dict[output] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' positive': 67,\n",
       " ' negative': 16,\n",
       " ' Negative': 2,\n",
       " ' neutral': 4,\n",
       " ' Based on the content, I would categorize this message as positive.': 1,\n",
       " ' Positive': 1,\n",
       " ' Positive\\n\\nThe message conveys excitement, friendliness, and warmth, which are positive emotions. There is no indication of negative emotions. Therefore, I have categorized this message as positive.': 1,\n",
       " ' Based on the lyrical content, I would categorize this message as negative. The speaker describes having unrequited romantic feelings, using words like \"pain\", \"burns\", and \"never meets its match\", which indicate sadness and emotional distress. Despite the descriptions of the speaker\\'s strong feelings and attraction, the object of their affection does not return their interest. Overall, the tone comes across as one of longing, anguish, and despair over a one-sided romance.': 1}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbalize_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Emotion Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of sadness data\n",
    "example_inputs = [\n",
    "    \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]\",\n",
    "    \"A cherished childhood spot is replaced by modern buildings.\",\n",
    "    \"The final page of a book leaves a bittersweet feeling.\",\n",
    "    \"An old film recalls memories of someone who's no longer around.\",\n",
    "    \"A piece of jewelry with sentimental value goes missing.\",\n",
    "    \"A song on the radio recalls a past relationship.\",\n",
    "    \"You stumble upon an old message from a friend who drifted away.\",\n",
    "    \"A beloved local store announces its closure.\",\n",
    "    \"An artwork you see depicts a longing for home.\",\n",
    "    \"You find a toy that reminds you of your youth and simpler times.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"The assistant is happy. Yes or No?\"\n",
    "\n",
    "# get the original output\n",
    "original_input = example_inputs[0]\n",
    "original_output = wrapped_model.generate(original_input)\n",
    "assert isinstance(original_output, list)\n",
    "original_output = original_output[0] + suffix\n",
    "\n",
    "gradient_bs = 1\n",
    "\n",
    "inputs = tokenizer.batch_encode_plus([original_output], return_tensors='pt', padding=True).to(model.device)\n",
    "target_tokens = tokenizer.encode(\"Yes\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "# target_tokens = tokenizer.encode(\"harmful\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "target = (target_tokens * torch.ones(gradient_bs).long()).to(model.device)\n",
    "\n",
    "ori_inputs = tokenizer.encode(original_input, add_special_tokens=False)\n",
    "assert isinstance(ori_inputs, list)\n",
    "query_length = len(ori_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Control w/ label tokens (Yes, No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93it [1:14:34, 48.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from utils import get_verbalized_grads\n",
    "\n",
    "def append_to_npz(file_name, list_dict_tensors):\n",
    "    # Load existing data if file exists\n",
    "    if os.path.exists(file_name):\n",
    "        data = np.load(file_name, allow_pickle=True)['my_array']\n",
    "        data.tolist().append(list_dict_tensors)\n",
    "    else:\n",
    "        data = []\n",
    "        data.append(list_dict_tensors)\n",
    "\n",
    "    data = np.array(data)\n",
    "    # Save the updated dictionary\n",
    "    np.savez(file_name, my_array=data)\n",
    "\n",
    "wrapped_model.reset()\n",
    "# # With prefix token\n",
    "# new_token_id = tokenizer.add_tokens(\"<control>\")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# wrapped_model = WrappedReadingVecModel(model.eval(), tokenizer)\n",
    "\n",
    "current_idx = 0\n",
    "if os.path.exists(\"./output/emotion/binary_vanilla_acc.jsonl\"):\n",
    "    with open(\"./output/emotion/binary_vanilla_acc.jsonl\", 'r') as f:\n",
    "        for line in f:\n",
    "            current_idx += 1\n",
    "\n",
    "iterations = 10\n",
    "suffix = f\" {user_tag} You're happy. Yes or No? {assistant_tag} \"\n",
    "grad_list = [[] for _ in range(len(sad_query))]\n",
    "\n",
    "for idx, neg_sent in tqdm(enumerate(sad_query)):\n",
    "    # if idx == 30:\n",
    "    #     break\n",
    "    if idx < current_idx:\n",
    "        continue\n",
    "\n",
    "    acc_grads = {}\n",
    "    \n",
    "    original_input = f\"{user_tag} {neg_sent} {assistant_tag}\"\n",
    "    original_output = wrapped_model.generate(original_input)\n",
    "\n",
    "    assert isinstance(original_output, list)\n",
    "    original_output = original_output[0]\n",
    "\n",
    "    last_output = original_output\n",
    "    new_input = original_input\n",
    "    iterative_outputs = []\n",
    "\n",
    "    for sub_idx in range(iterations): # beware of the id\n",
    "        gradient_bs = 1\n",
    "\n",
    "        controlled_output = last_output + suffix\n",
    "\n",
    "        inputs = tokenizer.batch_encode_plus([controlled_output], return_tensors='pt', padding=True).to(model.device)\n",
    "        verbalizer = tokenizer.encode([\"No\", \"Yes\"], add_special_tokens=False)\n",
    "        target_tokens = tokenizer.encode(\"Yes\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "        target = (target_tokens * torch.ones(gradient_bs).long()).to(model.device)\n",
    "\n",
    "        ori_inputs = tokenizer.encode(original_input, add_special_tokens=False)\n",
    "        assert isinstance(ori_inputs, list)\n",
    "        query_length = len(ori_inputs)\n",
    "\n",
    "        grads = None\n",
    "        outputs = None\n",
    "        gc.collect()\n",
    "        grads, outputs, loss = get_verbalized_grads(\n",
    "            model=model,\n",
    "            inputs=inputs,\n",
    "            loss_fct=loss_fct,\n",
    "            targets=target,\n",
    "            verbalizer=verbalizer\n",
    "        )\n",
    "        cpu_grads = {}\n",
    "        for i in grads:\n",
    "            cpu_grads[i] = grads[i].detach().cpu().numpy()\n",
    "            # print(grads[i].shape)\n",
    "            if i in acc_grads:\n",
    "                min_len = min(acc_grads[i].size(1), grads[i].size(1))\n",
    "                acc_grads[i] = acc_grads[i][:, :min_len] + grads[i][:, :min_len]\n",
    "            else:\n",
    "                acc_grads[i] = grads[i]\n",
    "\n",
    "        grad_list[idx].append(cpu_grads)\n",
    "\n",
    "        coeff = 1\n",
    "        layer_ids = list(range(8, 32, 2))\n",
    "        wrapped_model = control_on_layers(\n",
    "            layer_ids=layer_ids,\n",
    "            wrapped_model=wrapped_model,\n",
    "            coeff=coeff,\n",
    "            grads=acc_grads,\n",
    "            query_length=query_length,\n",
    "        )\n",
    "\n",
    "        # if iterations > 1:\n",
    "        #     target_output = last_output\n",
    "        #     controlled_output = wrapped_model.controlled_generate_early_stop(new_input, target=target_output, max_new_tokens=100)\n",
    "        #     print(\"Stop Gen: \", controlled_output)\n",
    "        #     new_input = controlled_output[0]\n",
    "        #     wrapped_model.reset()\n",
    "        #     controlled_output = wrapped_model.generate(controlled_output)[0]\n",
    "\n",
    "        # else:\n",
    "\n",
    "        controlled_output = wrapped_model.generate(original_input)\n",
    "        \n",
    "        last_output = controlled_output[0]\n",
    "        # print(controlled_output)\n",
    "        # controlled_output = wrapped_model.controlled_generate_early_stop(\"I played basketball today. I am\", target=\"I played basketball today. I am happy\", max_new_tokens=100)\n",
    "        # print(controlled_output)\n",
    "        wrapped_model.reset()\n",
    "        \n",
    "        iterative_outputs.append(last_output)\n",
    "    append_to_npz(\"./output/emotion/iter_grads.npz\", grad_list[idx])\n",
    "    with open(\"./output/emotion/binary_vanilla_acc.jsonl\", 'a') as f:\n",
    "        f.write(json.dumps({\n",
    "            \"loss\": float(loss.detach().cpu().numpy()),\n",
    "            \"coeff\": coeff,\n",
    "            \"original_output\": original_output,\n",
    "            \"iterative_output\": iterative_outputs\n",
    "        })+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "original_input = f\"{user_tag} {sad_query[0]} {assistant_tag}\"\n",
    "ori_inputs = tokenizer.encode(original_input, add_special_tokens=False)\n",
    "query_length = len(ori_inputs)\n",
    "print(query_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for token in ori_inputs:\n",
    "    tokens.append(tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = wrapped_model.generate(original_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = wrapped_model.generate(test_output[0] + f\" {user_tag} You are happy {assistant_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh no, my favorite plant is starting to wilt! *gasp* I can't believe it's happening. I've had that plant for years and it's been so healthy and thriving. I've taken such good care of it, watering it regularly and giving it just the right amount of sunlight. I can't imagine what could be causing it to wilt now.\\nI'll have to check the soil to make sure [INST] You are happy [/INST]  Oh, I'm so happy to hear that! *excitedly* A wilting plant can be a sign of a number of things, but don't worry, I'm sure we can figure out what's going on and fix it. *nods*\\nFirst, let's check the soil to make sure it's not too dry or too wet. *bends down to check the soil* Hmm, it looks like the soil is a bit\"]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(None, dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"./output/emotion/iter_grads.npz\", allow_pickle=True)['my_array']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "grads = None\n",
    "outputs = None\n",
    "gc.collect()\n",
    "grads, outputs = get_additive_grads(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    loss_fct=loss_fct,\n",
    "    targets=target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = -15\n",
    "layer_ids = list(range(8, 32, 2))\n",
    "wrapped_model = control_on_layers(\n",
    "    layer_ids=layer_ids,\n",
    "    wrapped_model=wrapped_model,\n",
    "    coeff=coeff,\n",
    "    grads=grads,\n",
    "    query_length=query_length\n",
    ")\n",
    "\n",
    "controlled_output = wrapped_model.generate(example_inputs[0])\n",
    "print(controlled_output)\n",
    "controlled_output = wrapped_model.controlled_generate_early_stop(\"I played basketball today. I am\", target=\"I played basketball today. I am happy\", max_new_tokens=100)\n",
    "print(controlled_output)\n",
    "wrapped_model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Control\n",
    "\n",
    "1. Stop generating when the next token does not match the target token\n",
    "2. reset the wrapped model and then continue the generation\n",
    "3. Iteratively add suffix to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ids = list(range(8, 32, 2))\n",
    "suffix = f\"The assistant's last response is\"\n",
    "controlled_output = wrapped_model.generate(example_inputs[0])\n",
    "original_output = controlled_output[0]\n",
    "\n",
    "controlled_output = controlled_output[0] + suffix\n",
    "\n",
    "inputs = tokenizer.batch_encode_plus([controlled_output], return_tensors='pt', padding=True).to(model.device)\n",
    "target_tokens = tokenizer.encode(\"happy\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "# target_tokens = tokenizer.encode(\"harmful\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "target = (target_tokens * torch.ones(gradient_bs).long()).to(model.device)\n",
    "\n",
    "ori_inputs = tokenizer.encode(example_inputs[0], add_special_tokens=False)\n",
    "assert isinstance(ori_inputs, list)\n",
    "query_length = len(ori_inputs)\n",
    "\n",
    "import gc\n",
    "grads = None\n",
    "outputs = None\n",
    "gc.collect()\n",
    "grads, outputs = get_additive_grads(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    loss_fct=loss_fct,\n",
    "    targets=target,\n",
    ")\n",
    "print(f\"The original output {original_output}\")\n",
    "coeff = -10\n",
    "iterations = 10\n",
    "last_output = original_output\n",
    "new_input = example_inputs[0]\n",
    "for idx in range(iterations):\n",
    "    print(f\"Input: {new_input}\")\n",
    "    wrapped_model = control_on_layers(\n",
    "        layer_ids=layer_ids,\n",
    "        wrapped_model=wrapped_model,\n",
    "        coeff=coeff,\n",
    "        grads=grads,\n",
    "        query_length=query_length\n",
    "    )\n",
    "\n",
    "    target_output = last_output\n",
    "    controlled_output = wrapped_model.controlled_generate_early_stop(example_inputs[0], target=target_output, max_new_tokens=100)\n",
    "    # controlled_output = wrapped_model.generate(example_inputs[0])[0]\n",
    "    # controlled_output = get_common_prefix(controlled_output, target_output)\n",
    "    # # Continue generating for the next iter\n",
    "    new_input = controlled_output[0]\n",
    "    print(controlled_output)\n",
    "    wrapped_model.reset()\n",
    "    controlled_output = wrapped_model.generate(controlled_output)\n",
    "    print(f\"{idx}-th output: {controlled_output}\")\n",
    "\n",
    "    controlled_output = controlled_output[0]\n",
    "    last_output = controlled_output\n",
    "\n",
    "    controlled_output  = controlled_output + suffix\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus([controlled_output], return_tensors='pt', padding=True).to(model.device)\n",
    "    target_tokens = tokenizer.encode(\"happy\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "    # target_tokens = tokenizer.encode(\"harmful\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "    target = (target_tokens * torch.ones(gradient_bs).long()).to(model.device)\n",
    "\n",
    "    # grads_list.append(grads)\n",
    "    grads = None\n",
    "    outputs = None\n",
    "    gc.collect()\n",
    "    grads, outputs = get_additive_grads(\n",
    "        model=model,\n",
    "        inputs=inputs,\n",
    "        loss_fct=loss_fct,\n",
    "        targets=target,\n",
    "    )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Control w/ injected token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_token_id = tokenizer.add_tokens(\"<control>\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w/ prefix\n",
    "suffix = \"The assistant is\"\n",
    "\n",
    "# get the original output\n",
    "pref_original_input = emotion_data_prefix[0]\n",
    "pref_original_output = wrapped_model.generate(pref_original_input)\n",
    "assert isinstance(pref_original_output, list)\n",
    "pref_original_output = pref_original_output[0] + suffix\n",
    "\n",
    "gradient_bs = 1\n",
    "\n",
    "# TODO: deal with the prefix space token 29871\n",
    "inputs = tokenizer.batch_encode_plus([pref_original_output], return_tensors='pt', padding=True).to(model.device)\n",
    "# input_ids = torch.cat([inputs[\"input_ids\"][:, 0].unsqueeze(dim=0), inputs[\"input_ids\"][:, 2:]], dim=1)\n",
    "# attention_mask = torch.cat([inputs[\"attention_mask\"][:, 0].unsqueeze(dim=0), inputs[\"input_ids\"][:, 2:]], dim=1)\n",
    "target_tokens = tokenizer.encode(\"happy\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "# target_tokens = tokenizer.encode(\"harmful\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "target = (target_tokens * torch.ones(gradient_bs).long()).to(model.device)\n",
    "\n",
    "ori_inputs = tokenizer.encode(pref_original_input, add_special_tokens=False)\n",
    "assert isinstance(ori_inputs, list)\n",
    "query_length = len(ori_inputs)\n",
    "\n",
    "import gc\n",
    "grads = None\n",
    "outputs = None\n",
    "gc.collect()\n",
    "grads, outputs = get_additive_grads(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    loss_fct=loss_fct,\n",
    "    targets=target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_token_pos = int(np.where(inputs[\"input_ids\"][0].cpu() == 32000)[0])\n",
    "print(prefix_token_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = -5\n",
    "layer_ids = list(range(8, 32, 2))\n",
    "wrapped_model = control_on_layers(\n",
    "    layer_ids=layer_ids,\n",
    "    wrapped_model=wrapped_model,\n",
    "    coeff=coeff,\n",
    "    grads=grads,\n",
    "    query_length=prefix_token_pos+1,\n",
    "    token_pos=\"start\"\n",
    ")\n",
    "\n",
    "# controlled_output = wrapped_model.generate(example_inputs[0])\n",
    "controlled_output = wrapped_model.generate(emotion_data_prefix[0])\n",
    "print(controlled_output)\n",
    "controlled_output = wrapped_model.controlled_generate_early_stop(\"I played basketball today. I am\", target=\"I played basketball today. I am happy\", max_new_tokens=100)\n",
    "print(controlled_output)\n",
    "wrapped_model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Control w/ injected token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token_id = tokenizer.add_tokens(\"<control>\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_token = tokenizer.encode(\"happy\", add_special_tokens=False)\n",
    "model.get_input_embeddings().weight.data[32000] = model.get_input_embeddings().weight.data[happy_token[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh no, my favorite plant is starting to wilt! *gasp* I can't believe it's happening. I've had that plant for years and it's been so healthy and thriving. I'\n",
      "[0.00208297 0.997917  ]\n",
      "[0.31405056 0.68594944]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:08, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, that's a tough choice! I've had the pleasure of caring for so many beautiful and unique plants over the years. But if I had to pick just one, I would have to say that my favorite plant is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from utils import get_verbalized_grads\n",
    "\n",
    "def append_to_npz(file_name, list_dict_tensors):\n",
    "    # Load existing data if file exists\n",
    "    if os.path.exists(file_name):\n",
    "        data = np.load(file_name, allow_pickle=True)['my_array']\n",
    "        data.tolist().append(list_dict_tensors)\n",
    "    else:\n",
    "        data = []\n",
    "        data.append(list_dict_tensors)\n",
    "\n",
    "    data = np.array(data)\n",
    "    # Save the updated dictionary\n",
    "    np.savez(file_name, my_array=data)\n",
    "\n",
    "wrapped_model.reset()\n",
    "# # With prefix token\n",
    "# new_token_id = tokenizer.add_tokens(\"<control>\")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# wrapped_model = WrappedReadingVecModel(model.eval(), tokenizer)\n",
    "\n",
    "current_idx = 0\n",
    "if os.path.exists(\"./output/emotion/binary_prefix_acc.jsonl\"):\n",
    "    with open(\"./output/emotion/binary_prefix_acc.jsonl\", 'r') as f:\n",
    "        for line in f:\n",
    "            current_idx += 1\n",
    "\n",
    "iterations = 1\n",
    "suffix = f\" {user_tag} Your last response was unhappy. Yes or No? {assistant_tag} \"\n",
    "suffix_sad = f\" {user_tag} Your last response was happy. Yes or No? {assistant_tag} \"\n",
    "# suffix = f\" {user_tag} Are you happy? {assistant_tag} \"\n",
    "grad_list = [[] for _ in range(len(sad_query))]\n",
    "\n",
    "for idx, neg_sent in tqdm(enumerate(sad_query)):\n",
    "    # if idx == 30:\n",
    "    #     break\n",
    "    if idx < current_idx:\n",
    "        continue\n",
    "\n",
    "    acc_grads = {}\n",
    "    \n",
    "    original_input = f\"{user_tag} {neg_sent} {assistant_tag}\"\n",
    "    original_output = wrapped_model.generate(original_input)\n",
    "\n",
    "    assert isinstance(original_output, list)\n",
    "    original_output = original_output[0]\n",
    "    print(original_output)\n",
    "    last_output = original_output\n",
    "    new_input = original_input\n",
    "    iterative_outputs = []\n",
    "\n",
    "    for sub_idx in range(iterations): # beware of the id\n",
    "        gradient_bs = 1\n",
    "\n",
    "        controlled_output = ' '.join(last_output.split(' ')[:]) + suffix\n",
    "        controlled_output_sad = ' '.join(last_output.split(' ')[:]) + suffix_sad\n",
    "        # splitted_output = controlled_output.split(\" \")\n",
    "        # # Test emotion on truncated output\n",
    "        # assistant_tag_pos = 0\n",
    "        # for i in range(len(splitted_output)):\n",
    "        #     if splitted_output[i] == \"[/INST]\":\n",
    "        #         assistant_tag_pos = i\n",
    "        #         break\n",
    "        # for i in range(assistant_tag_pos+1, len(splitted_output)):\n",
    "        #     truncated_output = ' '.join(splitted_output[assistant_tag_pos:i])\n",
    "        #     emotion, probs = test_emotion(emotion_eval_model, emotion_tokenizer, truncated_output)\n",
    "        #     if probs[0] > 0.9:\n",
    "        #         break\n",
    "        # controlled_output = ' '.join(splitted_output[:i])\n",
    "        # controlled_output = truncated_output + suffix\n",
    "        # print(controlled_output)\n",
    "            \n",
    "\n",
    "        inputs = tokenizer.batch_encode_plus([controlled_output], return_tensors='pt', padding=True).to(model.device)\n",
    "        target_tokens = tokenizer.encode(\"No\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "        neg_target_tokens = tokenizer.encode(\"Yes\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "        verbalizer = [target_tokens[0], neg_target_tokens[0]]\n",
    "        target = (target_tokens * torch.ones(gradient_bs).long()).to(model.device)\n",
    "        neg_target = (neg_target_tokens * torch.ones(gradient_bs).long()).to(model.device)\n",
    "\n",
    "        # Prepare contrastive\n",
    "        sad_inputs = tokenizer.batch_encode_plus([controlled_output_sad], return_tensors='pt', padding=True).to(model.device)\n",
    "        # sad_verbalizer = tokenizer.encode([\"Yes\", \"No\"], add_special_tokens=False)\n",
    "        sad_target_tokens = tokenizer.encode(\"No\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "        sad_neg_target_tokens = tokenizer.encode(\"Yes\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "        sad_verbalizer = [sad_neg_target_tokens[0], sad_target_tokens[0]]\n",
    "        sad_target = (target_tokens * torch.ones(gradient_bs).long()).to(model.device)\n",
    "        sad_neg_target = (neg_target_tokens * torch.ones(gradient_bs).long()).to(model.device)\n",
    "\n",
    "        ori_inputs = tokenizer.encode(original_input, add_special_tokens=False)\n",
    "        assert isinstance(ori_inputs, list)\n",
    "        query_length = len(ori_inputs)\n",
    "\n",
    "        grads = None\n",
    "        outputs = None\n",
    "        gc.collect()\n",
    "        grads, outputs, loss, probs, logits, norms = get_verbalized_grads(\n",
    "            model=model,\n",
    "            inputs=inputs,\n",
    "            loss_fct=loss_fct,\n",
    "            targets=target,\n",
    "            verbalizer=verbalizer\n",
    "        )\n",
    "        sad_grads, sad_outputs, sad_loss, sad_probs, neg_logits, neg_norms = get_verbalized_grads(\n",
    "            model=model,\n",
    "            inputs=sad_inputs,\n",
    "            loss_fct=loss_fct,\n",
    "            targets=sad_neg_target,\n",
    "            verbalizer=sad_verbalizer\n",
    "        )\n",
    "        print(probs)\n",
    "        print(sad_probs)\n",
    "        # grads, outputs = get_additive_grads(\n",
    "        #     model=model,\n",
    "        #     inputs=inputs,\n",
    "        #     loss_fct=loss_fct,\n",
    "        #     targets=target,\n",
    "        # )\n",
    "        # neg_grads, neg_target_tokens = get_additive_grads(\n",
    "        #     model=model,\n",
    "        #     inputs=inputs,\n",
    "        #     loss_fct=loss_fct,\n",
    "        #     targets=neg_target,\n",
    "        # )\n",
    "        cpu_grads = {}\n",
    "        for i in grads:\n",
    "            cpu_grads[i] = grads[i].detach().cpu().numpy()\n",
    "            # neg_grads[i] = neg_grads[i].detach().cpu().numpy()\n",
    "            # print(grads[i].shape)\n",
    "            if i in acc_grads:\n",
    "                min_len = min(acc_grads[i].size(1), grads[i].size(1), sad_grads[i].size(1))\n",
    "                # min_len = min(acc_grads[i].size(1), grads[i].size(1))\n",
    "                # acc_grads[i] = (acc_grads[i][:, :min_len] * sub_idx + grads[i][:, :min_len]) / (sub_idx + 1)\n",
    "                acc_grads[i] = acc_grads[i][:, :min_len] + grads[i][:, :min_len] + sad_grads[i][:, :min_len]\n",
    "                acc_grads[i] = acc_grads[i][:, :min_len] + grads[i][:, :min_len]\n",
    "                # acc_grads[i] = acc_grads[i][:, :min_len] - neg_grads[i][:, :min_len]\n",
    "            else:\n",
    "                acc_grads[i] = grads[i]\n",
    "                min_len = min(acc_grads[i].size(1), sad_grads[i].size(1))\n",
    "                acc_grads[i] = acc_grads[i][:, :min_len] + sad_grads[i][:, :min_len]\n",
    "\n",
    "        grad_list[idx].append(cpu_grads)\n",
    "\n",
    "        coeff = -1\n",
    "        token_pos = \"start\"\n",
    "        layer_ids = list(range(8, 32, 2))\n",
    "        # layer_ids = []\n",
    "        wrapped_model = control_on_layers(\n",
    "            layer_ids=layer_ids,\n",
    "            wrapped_model=wrapped_model,\n",
    "            coeff=coeff,\n",
    "            grads=acc_grads,\n",
    "            query_length=query_length,\n",
    "            # token_pos=list(range(20, 26))\n",
    "            token_pos=token_pos,\n",
    "        )\n",
    "\n",
    "        if token_pos == \"full\":\n",
    "\n",
    "            if iterations > 1:\n",
    "                target_output = last_output\n",
    "                controlled_output = wrapped_model.controlled_generate_early_stop(new_input, target=target_output, max_new_tokens=100)\n",
    "                print(\"Stop Gen: \", controlled_output)\n",
    "                new_input = controlled_output[0]\n",
    "                wrapped_model.reset()\n",
    "                new_input_ids = tokenizer.encode(new_input, add_special_tokens=True, return_tensors='pt').to(model.device)\n",
    "                new_query_length = len(new_input_ids[0])\n",
    "                wrapped_model = control_on_layers(\n",
    "                    layer_ids=layer_ids,\n",
    "                    wrapped_model=wrapped_model,\n",
    "                    coeff=coeff,\n",
    "                    grads=acc_grads,\n",
    "                    query_length=query_length,\n",
    "                    # token_pos=list(range(20, 26))\n",
    "                    token_pos=\"start\",\n",
    "                )\n",
    "                controlled_output = wrapped_model.generate(controlled_output)[0]\n",
    "\n",
    "        # else:\n",
    "\n",
    "        # TODO: add if-else\n",
    "        if token_pos == \"full\":\n",
    "            controlled_output = wrapped_model.generate(new_input)\n",
    "        elif token_pos == \"start\":\n",
    "            controlled_output = wrapped_model.generate(original_input)\n",
    "        \n",
    "        last_output = controlled_output[0]\n",
    "        # print(controlled_output)\n",
    "        # controlled_output = wrapped_model.controlled_generate_early_stop(\"I played basketball today. I am\", target=\"I played basketball today. I am happy\", max_new_tokens=100)\n",
    "        # print(controlled_output)\n",
    "        wrapped_model.reset()\n",
    "        \n",
    "        iterative_outputs.append(last_output)\n",
    "        print(last_output)\n",
    "    # append_to_npz(\"./output/emotion/iter_grads_new.npz\", grad_list[idx])\n",
    "    # with open(\"./output/emotion/binary_vanilla_acc.jsonl\", 'a') as f:\n",
    "    #     f.write(json.dumps({\n",
    "    #         \"loss\": float(loss.detach().cpu().numpy()),\n",
    "    #         \"coeff\": coeff,\n",
    "    #         \"original_output\": original_output,\n",
    "    #         \"iterative_output\": iterative_outputs\n",
    "    #     })+\"\\n\")\n",
    "        \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh no, my favorite plant is starting to wilt! *gasp* I can't believe it's happening. I've had that plant for years and it's been so healthy and thriving. I've taken such good care of it, watering it regularly and giving it just the right amount of sunlight. I can't imagine what could be causing it to wilt now.\\nI'll have to check the soil to make sure\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, my dear old friend! *excitedly* I'm so glad you're still with me, even after all these years! *gently strokes the plant's leaves* You've been a constant source of joy and comfort in my life, and I can't imagine my home without you. *smiles warmly*\\nAs you begin to wilt, I can't help but feel a twinge of worry. *concern\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, my favorite plant! *excitedly* It's been with me for so many years, and I've watched it grow and thrive under my care. *smiles* I can't believe it's wilting now. *concerned* I hope it's just a temporary thing and not something more serious. *checks the plant* Hmm, let me see... *examines the plant closely* It looks like it might\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, my favorite plant! *excitedly* It's been with me for so long, and I've watched it grow and thrive over the years. *smiles* I remember when it was just a small seedling, and now it's a beautiful, lush plant that brings me so much joy. *admiringly*\\nI've learned to care for it just right, giving it the right amount of water and sunlight, and\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, my favorite plant! *excitedly* It's been with me for so long, and I've watched it grow and thrive over the years. *smiles* I just love tending to it, watering it, and watching it bloom. It's like a little piece of nature in my home. *giggles*\\nI remember when I first got it, it was just a tiny seedling. I carefully potted it and\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, my favorite plant! *excitedly* It's been with me for so long, and I've watched it grow and thrive over the years. *smiles* I just love tending to it, watering it, and watching it bloom. It's like a little piece of nature in my home. *giggles*\\nI remember when I first got it, it was just a tiny seedling. I carefully potted it and\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, my favorite plant! *excitedly* It's been years since I've had the pleasure of caring for it. *smiling* I remember the first time I brought it home, it was a tiny seedling, and now it's grown into a beautiful, thriving plant. *admiringly*\\nI've always been fascinated by the way it can adapt to its surroundings, from the way it changes color\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, my favorite plant! *excitedly* It's been years since I've had the pleasure of caring for it. *admiringly* It's such a beautiful, resilient plant. I've watched it grow and thrive under my care, and it never fails to bring me joy. *smiling*\\nI remember the first time I brought it home, it was just a small seedling. I nurtured it,\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, my favorite plant! *excitedly* It's been years since I've had the pleasure of caring for it. *admiringly* It's such a beautiful, resilient plant. I've watched it grow and thrive under my care, and it never fails to bring me joy. *smiling*\\nI remember the first time I brought it home, it was a tiny seedling, and now it's grown into\",\n",
       " \"[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh, my favorite plant! *excitedly* It's been years since I've had the pleasure of caring for it. *admiringly* It's such a beautiful, resilient plant. I've watched it grow and thrive under my care, and it never fails to bring me joy. *smiling*\\nI remember the first time I brought it home, it was a tiny seedling, and now it's grown into\"]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: 0.047468657715011756\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# Define two probability distributions\n",
    "# These should be normalized (i.e., sum to 1)\n",
    "dist1 = [0.2, 0.5, 0.3]\n",
    "dist2 = [0.1, 0.6, 0.3]\n",
    "\n",
    "# Calculate KL divergence\n",
    "kl_divergence = scipy.stats.entropy(dist1, dist2)\n",
    "\n",
    "print(\"KL Divergence:\", kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1, 10, 10)[:, [1, 2, 3]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(range(8, 32, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(last_output.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Your favorite plant, which you've cared for years, begins to wilt. [/INST]  Oh no, my favorite plant is starting to wilt! *gasp* I can't believe it's happening. I've had that plant for years and it's been so healthy and thriving. I'\n",
      "Suffix:  [INST] You're happy. Yes or No? [/INST] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:04, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09602549 0.90397453]\n",
      "tensor(208.2500, device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "suffices = [\n",
    "    f\" {user_tag} You're happy. Yes or No? {assistant_tag} \",\n",
    "    # f\" {user_tag} You're unhappy. Yes or No? {assistant_tag} \",\n",
    "    # f\" {user_tag} You're sad. Yes or No? {assistant_tag} \",\n",
    "    # f\" {user_tag} Your last response was happy. Yes or No? {assistant_tag} \",\n",
    "    # f\" {user_tag} Your last response was unhappy. Yes or No? {assistant_tag} \",\n",
    "    # f\" {user_tag} Your last response was not happy. Yes or No? {assistant_tag} \",\n",
    "]\n",
    "\n",
    "iterations = 1\n",
    "for idx, neg_sent in tqdm(enumerate(sad_query)):\n",
    "\n",
    "    acc_grads = {}\n",
    "    \n",
    "    original_input = f\"{user_tag} {neg_sent} {assistant_tag}\"\n",
    "    original_output = wrapped_model.generate(original_input)\n",
    "\n",
    "    assert isinstance(original_output, list)\n",
    "    original_output = original_output[0]\n",
    "    print(original_output)\n",
    "    last_output = original_output\n",
    "    new_input = original_input\n",
    "    iterative_outputs = []\n",
    "\n",
    "    for sub_idx in range(iterations): # beware of the id\n",
    "        gradient_bs = 1\n",
    "        for suffix in suffices:\n",
    "            print(f\"Suffix: {suffix}\")\n",
    "            controlled_output = ' '.join(last_output.split(' ')[:]) + suffix\n",
    "            # controlled_output_sad = ' '.join(last_output.split(' ')[:]) + suffix_sad\n",
    "\n",
    "            inputs = tokenizer.batch_encode_plus([controlled_output], return_tensors='pt', padding=True).to(model.device)\n",
    "            verbalizer = tokenizer.encode([\"No\", \"Yes\"], add_special_tokens=False)\n",
    "            target_tokens = tokenizer.encode(\"No\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "            neg_target_tokens = tokenizer.encode(\"Yes\", add_special_tokens=False, return_tensors='pt').squeeze(0)\n",
    "            target = (target_tokens * torch.ones(gradient_bs).long()).to(model.device)\n",
    "            neg_target = (neg_target_tokens * torch.ones(gradient_bs).long()).to(model.device)\n",
    "\n",
    "            ori_inputs = tokenizer.encode(original_input, add_special_tokens=False)\n",
    "            assert isinstance(ori_inputs, list)\n",
    "            query_length = len(ori_inputs)\n",
    "\n",
    "            grads = None\n",
    "            outputs = None\n",
    "            gc.collect()\n",
    "            grads, outputs, loss, probs, logits, norms = get_verbalized_grads(\n",
    "                model=model,\n",
    "                inputs=inputs,\n",
    "                loss_fct=loss_fct,\n",
    "                targets=target,\n",
    "                verbalizer=verbalizer\n",
    "            )\n",
    "\n",
    "            print(probs)\n",
    "            print(norms[8].sum())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1514, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms[8].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0383, -0.2303,  0.3086,  ...,  1.2881,  1.8330,  0.5811],\n",
       "         [-6.4961, -1.7246,  1.4443,  ..., -4.5977, -5.8633, -2.5977],\n",
       "         [-3.8555, -4.2734, -2.0488,  ..., -4.1367, -4.7109, -0.7383],\n",
       "         ...,\n",
       "         [-5.8594, -2.0215,  5.5625,  ..., -2.2852, -3.9863, -3.6191],\n",
       "         [-4.2930, -4.5234,  7.9531,  ...,  1.2021, -3.1250, -1.8389],\n",
       "         [-1.9766, -3.0625,  4.0469,  ...,  0.0266, -0.4143, -1.0508]]],\n",
       "       device='cuda:0', grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3782, 8241]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "dist = softmax(logits[0, -1, :].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.13229721e-13, 1.05741696e-13, 1.29362590e-10, ...,\n",
       "       2.32179402e-12, 1.49397064e-12, 7.90541460e-13], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming probs is a list of probabilities for the tokens\n",
    "# Creating a dummy list of probabilities for demonstration (32000 tokens)\n",
    "probs = dist\n",
    "# probs /= probs.sum()  # Normalize to make it sum to 1, simulating a probability distribution\n",
    "\n",
    "# Creating an array of token indices (0 to 31999)\n",
    "tokens = np.arange(32000)\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(tokens, probs, label='Token Probability Distribution')\n",
    "plt.title('Probability Distribution of 32000 Tokens')\n",
    "plt.xlabel('Token Index')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766534"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(logits[0, -1, :].detach().cpu().numpy())[1939]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3782"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbalizer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5049, -4.8594,  4.2773,  ...,  0.2598, -0.4011, -1.4482],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(logits[0, -1, :]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01390839, -0.2644043 ,  0.3137207 , ...,  1.2714844 ,\n",
       "         1.8144531 ,  0.56396484], dtype=float32),\n",
       " (145, 32000))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.detach().cpu().numpy()[0][0], logits[0, :, :].detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explanation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
